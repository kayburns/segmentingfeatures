<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Spatial features predict robust transfer in pre-trained visual representations.">
  <meta name="keywords" content="Spatial-Features, PVR Analysis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.plot.ly/plotly-2.26.0.min.js" charset="utf-8"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://kayburns.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://sites.google.com/stanford.edu/adaptive-nstep-returns">
            Adaptive N-Step
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kayburns.github.io">Kaylee Burns</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Zach Witzel</a><sup>1</sup>,</span>
            <span class="author-block">
              <a>Jubayer Ibn Hamid</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://karolhausman.github.io">Karol Hausman</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Stanford University,</span>
            <span class="author-block"><sup>2</sup>Google DeepMind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/stanford-iris-lab/spatial_feats/tree/eval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <!-- <img src="./media/problem.gif" width="1000%"/> -->
      <video id="teaser" autoplay muted loop playsinline height="60%">
        <source src="./static/videos/trailer.mp4"
                type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2> -->
    </div>
  </div>
</section>

<!-- successful in 6/10 -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3"> What predicts the generalizability of a Pre-Trained Visual Representation (PVR)? </h2>
      <p>We find that the emergent segmentation performance of a ViT model, 
        which we refer to as <b>Spatial Features</b>, strongly predicts generalization performance.
        In the example below, we visualize the attention of a ViT that has a high emergent segmentation performance:
        MoCo-v3. Policies trained on top of this model are successful zero-shot in 6 out of the 10 distribution shifts depicted below.
      </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item image-container">
          <img class="blue-border" src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_train.gif" width="100%"/>
          <div class="hover-label">Train distribution</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_dark-woodtable.gif" width="100%"/>
          <div class="hover-label">Dark wood texture</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_distractor_hard.gif" width="100%"/>
          <div class="hover-label">9 disctractors</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_blue-woodtable.gif" width="100%"/>
          <div class="hover-label">Blue wood texture</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_brighter.gif" width="100%"/>
          <div class="hover-label">Brighter lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_cast_left.gif" width="100%"/>
          <div class="hover-label">Cast left lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_cast_right.gif" width="100%"/>
          <div class="hover-label">Cast right lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_darker.gif" width="100%"/>
          <div class="hover-label">Darker lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_darkwoodtable.gif" width="100%"/>
          <div class="hover-label">Darker wood texture</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_distractor_easy.gif" width="100%"/>
          <div class="hover-label">1 distractor</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/moco/shift_heatmap_vid_0_1_sawyer_hammer_distractor_medium.gif" width="100%"/>
          <div class="hover-label">3 distractors</div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- successful in 3/10 -->
<section class="hero is-light is-small"> 
  <div class="hero-body">
    <div class="container">
      <p>Models that don't have spatial features, such as Masked Visual Pre-training (MVP), are less successful at generalizing
        to new visual appearances. In the example below, a policy learned on top of MVP only generalizes to 3
        out of 10 visual shifts. This is surprising because MVP is designed for manipulation and control tasks.
       </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item image-container">
          <img class="blue-border" src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_train.gif" width="100%"/>
          <div class="hover-label">Train distribution</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_dark-woodtable.gif" width="100%"/>
          <div class="hover-label">Dark wood texture</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_distractor_hard.gif" width="100%"/>
          <div class="hover-label">9 disctractors</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_blue-woodtable.gif" width="100%"/>
          <div class="hover-label">Blue wood texture</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_brighter.gif" width="100%"/>
          <div class="hover-label">Brighter lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_cast_left.gif" width="100%"/>
          <div class="hover-label">Cast left lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_cast_right.gif" width="100%"/>
          <div class="hover-label">Cast right lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_darker.gif" width="100%"/>
          <div class="hover-label">Darker lighting</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_darkwoodtable.gif" width="100%"/>
          <div class="hover-label">Darker wood texture</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_distractor_easy.gif" width="100%"/>
          <div class="hover-label">1 distractor</div>
        </div>
        <div class="item image-container">
          <img src="./media/hammer_right_cap2/mvp/shift_heatmap_vid_0_0_sawyer_hammer_distractor_medium.gif" width="100%"/>
          <div class="hover-label">3 distractors</div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Different Metrics of Generalizability</h2>
    <p>
      We can evaluate the example above quantitatively. Specifically we measure the predictive power
      of spatial features by correlating the Jaccard index of the attention heads of different 
      pre-trained models with the out-of-distribution performance of a downstream policy.
      This metric is more predictive than other metrics of generalizability, such as downstream ImageNet accuracy,
      in-domain accuracy, or shape-bias as evaluated by cue-conflict performance.
    </p>
    <!-- add some space between the previous paragraph and the plots below -->
    <br>

    <div class="columns is-centered">

      <!-- Shape-bias. -->
      <div class="column">
        <h3 class="title is-4">In-Domain vs. OOD</h3>
        <div id="indomain"> </div>
      </div>
      <!--/ Shape-bias. -->

      <!-- ImageNet. -->
      <div class="column">
        <h3 class="title is-4">Imagenet vs. OOD</h3>
        <div id="imagenet"> </div>
      </div>
      <!--/ ImageNet. -->

    </div>
    <div class="columns is-centered">

      <!-- Shape-bias. -->
      <div class="column">
        <h3 class="title is-4">Shape-Bias vs. OOD</h3>
        <div id="shape"> </div>
      </div>
      <!--/ Shape-bias. -->

      <!-- Jaccard. -->
      <div class="column">
        <h3 class="title is-4">Jaccard Index vs. OOD</h3>
        <div id="jaccard"> </div>
      </div>
      <!--/ Jaccard. -->

    </div>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Inspired by the success of transfer learning in computer vision, roboticists have investigated visual pre-training as
                a means to improve the learning efficiency and generalization ability of policies learned from pixels.
                To that end, past work has favored large object interaction datasets, such as first-person videos of humans completing
                diverse tasks, in pursuit of manipulation-relevant features.
                Although this approach improves the efficiency of policy learning, it remains unclear how reliable these
                representations are in the presence of distribution shifts that arise commonly in robotic applications.
                Surprisingly, we find that visual representations designed for manipulation and control tasks do not necessarily
                generalize under subtle changes in lighting and scene texture or the introduction of distractor objects.
                To understand what properties <em>do</em> lead to robust representations, we compare the performance of 15 pre-trained
                vision models under different visual appearances.
                We find that emergent segmentation ability is a strong predictor of out-of-distribution generalization among ViT
                models.
                The rank order induced by this metric is more predictive than metrics that have previously guided generalization
                research within computer vision and machine learning, such as downstream ImageNet accuracy, in-domain accuracy, or
                shape-bias as evaluated by cue-conflict performance.
                We test this finding extensively on a suite of distribution shifts in ten tasks across two simulated manipulation
                environments. On the ALOHA setup, segmentation score predicts real-world performance after offline training with 50
                demonstrations.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    
      </div>
    </section>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent on pre-trained visual representations. This work wouldn't have been possible without 
            the development and open-sourcing of existing PVR models such as 
            <a
              href="https://research.facebook.com/publications/vip-towards-universal-visual-reward-and-representation-via-value-implicit-pretraining/">VIP</a>,
            <a href="https://tetexiao.com/projects/real-mvp">MVP</a>, and 
            <a href="https://github.com/facebookresearch/r3m">R3M</a>.
            Please also see concurrent work that studies <a href="https://data4robotics.github.io"> visual pre-training for robotics</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{Burns2023WhatMakesPVR,
  author    = {Burns, Kaylee and Witzel, Zach and Hamid, Jubayer Ibn and Yu, Tianhe and Finn, Chelsea and Hausman, Karol},
  title     = {What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?},
  journal   = {ArXiv},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/spatialfeatures_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/kayburns" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is borrowed from the <a href="https://nerfies.github.io">Nerfies</a> paper. Please find the source code
            and details about adapting the website template <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
<script src="./static/js/plot.js"></script>
</html>
